<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>spider-py</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="index.html">Introduction</a></li><li class="chapter-item expanded affix "><li class="part-title">User Guide</li><li class="chapter-item expanded "><a href="getting-started.html"><strong aria-hidden="true">1.</strong> Getting started</a></li><li class="chapter-item expanded "><a href="simple.html"><strong aria-hidden="true">2.</strong> A simple example</a></li><li class="chapter-item expanded affix "><li class="part-title">Configuration</li><li class="chapter-item expanded "><a href="website.html"><strong aria-hidden="true">3.</strong> Website</a></li><li class="chapter-item expanded "><a href="page.html"><strong aria-hidden="true">4.</strong> Page</a></li><li class="chapter-item expanded "><a href="env.html"><strong aria-hidden="true">5.</strong> Environment</a></li><li class="chapter-item expanded affix "><li class="part-title">Usage</li><li class="chapter-item expanded "><a href="crawl.html"><strong aria-hidden="true">6.</strong> Crawl</a></li><li class="chapter-item expanded "><a href="scrape.html"><strong aria-hidden="true">7.</strong> Scrape</a></li><li class="chapter-item expanded "><a href="cron-job.html"><strong aria-hidden="true">8.</strong> Cron Job</a></li><li class="chapter-item expanded "><a href="storing-data.html"><strong aria-hidden="true">9.</strong> Storing Data</a></li><li class="chapter-item expanded affix "><li class="part-title">Benchmarks</li><li class="chapter-item expanded "><a href="benchmarks.html"><strong aria-hidden="true">10.</strong> Compare</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">spider-py</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/spider-rs/spider-py/tree/main/book" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p><code>Spider-Py</code> is the fastest web crawler and indexer written in Rust ported to Python.</p>
<ul>
<li>Concurrent</li>
<li>Streaming</li>
<li>Decentralization</li>
<li>Headless Chrome <a href="https://github.com/mattsse/chromiumoxide">Rendering</a></li>
<li>HTTP Proxies</li>
<li>Cron Jobs</li>
<li>Subscriptions</li>
<li>Blacklisting and Budgeting Depth</li>
<li>Written in <a href="https://www.rust-lang.org/">Rust</a> for speed, safety, and simplicity</li>
</ul>
<p>Spider powers some big tools and helps bring the crawling aspect to almost no downtime with the correct setup, view the <a href="https://github.com/spider-rs/spider">spider</a> project to learn more.</p>
<p>Test url: <code>https://espn.com</code></p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>pages</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider-rs(python): crawl</code></strong></td><td style="text-align: left"><code>150,387</code></td><td style="text-align: left"><code>186s</code></td></tr>
<tr><td style="text-align: left"><strong><code>scrapy(python): crawl</code></strong></td><td style="text-align: left"><code>49,598</code></td><td style="text-align: left"><code>1h</code></td></tr>
</tbody></table>
</div>
<p>The benches above were ran on a mac m1, spider on linux arm machines performs about 2-10x faster.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h1>
<p>Make sure to have python installed.</p>
<pre><code class="language-sh">pip install spider_rs
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="simple-example"><a class="header" href="#simple-example">Simple Example</a></h1>
<p>We use the <a href="https://pyo3.rs/v0.20.0/">pyo3</a> to port the Rust project to target Python.</p>
<p>There are some performance drawbacks from the addon, even still the crawls are lightning fast and efficient.</p>
<h2 id="usage"><a class="header" href="#usage">Usage</a></h2>
<p>The examples below can help get started with spider.</p>
<h3 id="basic"><a class="header" href="#basic">Basic</a></h3>
<pre><code class="language-python">import asyncio

from spider_rs import Website

async def main():
    website = Website(&quot;https://jeffmendez.com&quot;)
    website.crawl()
    print(website.links)
    # print(website.pages)

asyncio.run(main())
</code></pre>
<h3 id="events"><a class="header" href="#events">Events</a></h3>
<p>You can pass an object that could be async as param to <code>crawl</code> and <code>scrape</code>.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

class Subscription:
    def __init__(self):
        print(&quot;Subscription Created...&quot;)
    def __call__(self, page):
        print(page.url + &quot; - status: &quot; + str(page.status_code))

async def main():
    website = Website(&quot;https://choosealicense.com&quot;)
    website.crawl(Subscription())

asyncio.run(main())
</code></pre>
<h3 id="selector"><a class="header" href="#selector">Selector</a></h3>
<p>The <code>title</code> method allows you to extract the title of the page.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

class Subscription:
    def __init__(self):
        print(&quot;Subscription Created...&quot;)
    def __call__(self, page):
        print(page.url + &quot; - title: &quot; + str(page.title()))

async def main():
    website = Website(&quot;https://choosealicense.com&quot;)
    website.crawl(Subscription())
</code></pre>
<h2 id="shortcut"><a class="header" href="#shortcut">Shortcut</a></h2>
<p>You can use the <code>crawl</code> shortcut method to collect contents quickly without configuration.</p>
<pre><code class="language-ts">import asyncio

from spider_rs import crawl

async def main():
    website = crawl(&quot;https://jeffmendez.com&quot;)
    print(website.links)
    # print(website.pages)

asyncio.run(main())
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="website"><a class="header" href="#website">Website</a></h1>
<p>The Website class is the foundations to the spider.</p>
<h2 id="builder-pattern"><a class="header" href="#builder-pattern">Builder pattern</a></h2>
<p>We use the builder pattern to configure the website for crawling.</p>
<p>*note: Replace <code>https://choosealicense.com</code> from the examples below with your website target URL.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;)
    website.crawl()
    print(website.get_links())

asyncio.run(main())
</code></pre>
<h3 id="custom-headers"><a class="header" href="#custom-headers">Custom Headers</a></h3>
<p>Add custom HTTP headers to use when crawling/scraping.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;).with_headers({ &quot;authorization&quot;: &quot;mytoken&quot;})

asyncio.run(main())
</code></pre>
<h3 id="blacklist"><a class="header" href="#blacklist">Blacklist</a></h3>
<p>Prevent crawling a set path, url, or pattern with Regex.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;).with_blacklist_url([&quot;/blog&quot;, &quot;/resume&quot;])

asyncio.run(main())
</code></pre>
<h3 id="crons"><a class="header" href="#crons">Crons</a></h3>
<p>Setup a cron job that can run at any time in the background using cron-syntax.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;).with_cron(&quot;1/5 * * * * *&quot;)

asyncio.run(main())
</code></pre>
<p>View the <a href="./cron-job.html">cron</a> section for details how to use the cron.</p>
<h3 id="budget"><a class="header" href="#budget">Budget</a></h3>
<p>Add a crawl budget that prevents crawling <code>x</code> amount of pages.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;).with_budget({
    &quot;*&quot;: 1,
  })

asyncio.run(main())
</code></pre>
<h3 id="subdomains"><a class="header" href="#subdomains">Subdomains</a></h3>
<p>Include subdomains in request.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;).with_subdomains(True)

asyncio.run(main())
</code></pre>
<h3 id="tld"><a class="header" href="#tld">TLD</a></h3>
<p>Include TLDs in request.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;).with_tlds(True)

asyncio.run(main())
</code></pre>
<h3 id="external-domains"><a class="header" href="#external-domains">External Domains</a></h3>
<p>Add external domains to include with the website.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;).with_external_domains([&quot;https://www.myotherdomain.com&quot;])

asyncio.run(main())
</code></pre>
<h3 id="proxy"><a class="header" href="#proxy">Proxy</a></h3>
<p>Use a proxy to crawl a website.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;).with_proxies([&quot;https://www.myproxy.com&quot;])

asyncio.run(main())
</code></pre>
<h3 id="depth-limit"><a class="header" href="#depth-limit">Depth Limit</a></h3>
<p>Set the depth limit for the amount of forward pages.</p>
<pre><code class="language-ts">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;).with_depth(3)

asyncio.run(main())
</code></pre>
<h3 id="cache"><a class="header" href="#cache">Cache</a></h3>
<p>Enable HTTP caching, this useful when using the spider on a server.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;).with_caching(True)

asyncio.run(main())
</code></pre>
<h3 id="delays"><a class="header" href="#delays">Delays</a></h3>
<p>Add delays between pages. Defaults to none.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;).with_delays(200)

asyncio.run(main())
</code></pre>
<h3 id="user-agent"><a class="header" href="#user-agent">User-Agent</a></h3>
<p>Use a custom User-Agent.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;).with_user_agent(&quot;mybot/v1&quot;)

asyncio.run(main())
</code></pre>
<h3 id="request-timeout"><a class="header" href="#request-timeout">Request Timeout</a></h3>
<p>Add a request timeout per page in miliseconds. Example shows 30 seconds.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;).with_request_timeout(30000)

asyncio.run(main())
</code></pre>
<h3 id="respect-robots"><a class="header" href="#respect-robots">Respect Robots</a></h3>
<p>Respect the robots.txt file.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;).with_respect_robots_txt(True)

asyncio.run(main())
</code></pre>
<h3 id="http2-prior-knowledge"><a class="header" href="#http2-prior-knowledge">Http2 Prior Knowledge</a></h3>
<p>Use http2 to connect if you know the website servers supports this.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;).with_http2_prior_knowledge(True)

asyncio.run(main())
</code></pre>
<h2 id="chaining"><a class="header" href="#chaining">Chaining</a></h2>
<p>You can chain all of the configs together for simple configuration.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;).with_subdomains(true).with_tlds(true).with_user_agent(&quot;mybot/v1&quot;).with_respect_robots_txt(true)

asyncio.run(main())
</code></pre>
<h2 id="raw-content"><a class="header" href="#raw-content">Raw Content</a></h2>
<p>Set the second param of the website constructor to <code>true</code> to return content without UTF-8.
This will return <code>rawContent</code> and leave <code>content</code> when using subscriptions or the Page Object.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;, True)
    website.scrape()

asyncio.run(main())
</code></pre>
<h2 id="clearing-crawl-data"><a class="header" href="#clearing-crawl-data">Clearing Crawl Data</a></h2>
<p>Use <code>website.clear</code> to remove the links visited and page data or <code>website.drain_links</code> to drain the links visited.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;)
    website.crawl()
    print(website.getLinks())
    website.clear()
    print(website.getLinks())

asyncio.run(main())
</code></pre>
<h2 id="stop-crawl"><a class="header" href="#stop-crawl">Stop crawl</a></h2>
<p>To stop a crawl you can use <code>website.stopCrawl(id)</code>, pass in the crawl id to stop a run or leave empty for all crawls to stop.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

class Subscription:
    def __init__(self):
        print(&quot;Subscription Created...&quot;)
    def __call__(self, page):
        print(page.url + &quot; - status: &quot; + str(page.status_code))

async def main():
    website = Website(&quot;https://choosealicense.com&quot;)
    website.crawl(Subscription())
    # sleep for 2s and stop etc
    website.stop()

asyncio.run(main())
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="page"><a class="header" href="#page">Page</a></h1>
<p>A single page on a website, useful if you need just the root url.</p>
<h2 id="new-page"><a class="header" href="#new-page">New Page</a></h2>
<p>Get a new page with content.</p>
<p>The first param is the url, followed by if subdomains should be included, and last to include TLD's in links.</p>
<p>Calling <code>page.fetch</code> is needed to get the content.</p>
<pre><code class="language-python">import asyncio
from spider_rs import Page

async def main():
    page = Page(&quot;https://choosealicense.com&quot;)
    page.fetch()

asyncio.run(main())
</code></pre>
<h2 id="page-links"><a class="header" href="#page-links">Page Links</a></h2>
<p>get all the links related to a page.</p>
<pre><code class="language-python">import asyncio
from spider_rs import Page

async def main():
    page = Page(&quot;https://choosealicense.com&quot;)
    page.fetch()
    links = page.get_links()
    print(links)
asyncio.run(main())
</code></pre>
<h2 id="page-html"><a class="header" href="#page-html">Page Html</a></h2>
<p>Get the markup for the page or HTML.</p>
<pre><code class="language-python">import asyncio
from spider_rs import Page

async def main():
    page = Page(&quot;https://choosealicense.com&quot;)
    page.fetch()
    links = page.get_html()
    print(links)

asyncio.run(main())
</code></pre>
<h2 id="page-bytes"><a class="header" href="#page-bytes">Page Bytes</a></h2>
<p>Get the raw bytes of a page to store the files in a database.</p>
<pre><code class="language-python">import asyncio
from spider_rs import Page

async def main():
    page = Page(&quot;https://choosealicense.com&quot;)
    page.fetch()
    links = page.get_bytes()
    print(links)

asyncio.run(main())
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="environment"><a class="header" href="#environment">Environment</a></h1>
<p>Env variables to adjust the project.</p>
<h2 id="chrome_url"><a class="header" href="#chrome_url">CHROME_URL</a></h2>
<p>You can set the chrome URL to connect remotely.</p>
<pre><code class="language-sh">CHROME_URL=http://localhost:9222
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="crawl"><a class="header" href="#crawl">Crawl</a></h1>
<p>Crawl a website concurrently.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://rsseau.fr&quot;)
    website.crawl()
    print(website.get_links())

asyncio.run(main())
</code></pre>
<h2 id="async-event"><a class="header" href="#async-event">Async Event</a></h2>
<p>You can pass in a async function as the first param to the crawl function for realtime updates streamed.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

class Subscription:
    def __init__(self):
        print(&quot;Subscription Created...&quot;)
    def __call__(self, page):
        print(page.url + &quot; - status: &quot; + str(page.status_code))

async def main():
    website = Website(&quot;https://choosealicense.com&quot;)
    website.crawl(Subscription())

asyncio.run(main())
</code></pre>
<h2 id="background"><a class="header" href="#background">Background</a></h2>
<p>You can run the request in the background and receive events with the second param set to <code>true</code>.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

class Subscription:
    def __init__(self):
        print(&quot;Subscription Created...&quot;)
    def __call__(self, page):
        print(page.url + &quot; - status: &quot; + str(page.status_code))

async def main():
    website = Website(&quot;https://choosealicense.com&quot;)
    website.crawl(Subscription(), True)
    # this will run instantly as the crawl is in the background

asyncio.run(main())
</code></pre>
<h2 id="subscriptions"><a class="header" href="#subscriptions">Subscriptions</a></h2>
<p>You can setup many subscriptions to run events when a crawl happens.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

class Subscription:
    def __init__(self):
        print(&quot;Subscription Created...&quot;)
    def __call__(self, page):
        print(page.url + &quot; - status: &quot; + str(page.status_code))

async def main():
    website = Website(&quot;https://choosealicense.com&quot;)
    website.crawl()
    subscription_id = website.subscribe(Subscription());
    website.crawl()
    website.unsubscribe(subscription_id);

asyncio.run(main())
</code></pre>
<h2 id="headless-chrome"><a class="header" href="#headless-chrome">Headless Chrome</a></h2>
<p>Headless Chrome rendering can be done by setting the third param in <code>crawl</code> or <code>scrape</code> to <code>true</code>.
It will attempt to connect to chrome running remotely if the <code>CHROME_URL</code> env variable is set with chrome launching as a fallback. Using a remote connection with <code>CHROME_URL</code> will
drastically speed up runs.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

class Subscription:
    def __init__(self):
        print(&quot;Subscription Created...&quot;)
    def __call__(self, page):
        print(page.url + &quot; - status: &quot; + str(page.status_code))

async def main():
    website = Website(&quot;https://choosealicense.com&quot;)
    website.crawl(Subscription(), false, True)

asyncio.run(main())
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scrape"><a class="header" href="#scrape">Scrape</a></h1>
<p>Scape a website and collect the resource data.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;)
    website.scrape()
    print(website.get_pages())
    # [ { url: &quot;https://rsseau.fr/blog&quot;, html: &quot;&lt;html&gt;...&lt;/html&gt;&quot;}, ...]

asyncio.run(main())
</code></pre>
<h2 id="headless-chrome-1"><a class="header" href="#headless-chrome-1">Headless Chrome</a></h2>
<p>Headless Chrome rendering can be done by setting the third param in <code>crawl</code> or <code>scrape</code> to <code>true</code>.
It will attempt to connect to chrome running remotely if the <code>CHROME_URL</code> env variable is set with chrome launching as a fallback. Using a remote connection with <code>CHROME_URL</code> will
drastically speed up runs.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website(&quot;https://choosealicense.com&quot;)
    website.scrape(NULL, NULL, True)
    print(website.get_pages())
    # [ { url: &quot;https://rsseau.fr/blog&quot;, html: &quot;&lt;html&gt;...&lt;/html&gt;&quot;}, ...]

asyncio.run(main())
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cron-jobs"><a class="header" href="#cron-jobs">Cron Jobs</a></h1>
<p>Use a cron job that can run any time of day to gather website data.</p>
<pre><code class="language-python">import asyncio
from spider_rs import Website

class Subscription:
    def __init__(self):
        print(&quot;Cron Created...&quot;)
    def __call__(self, page):
        print(page.url + &quot; - status: &quot; + str(page.status_code))

async def main():
    website = Website(&quot;https://choosealicense.com&quot;).with_cron(&quot;1/5 * * * * *&quot;).build()
    handle = await website.run_cron(Subscription());

asyncio.run(main())
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="storing-data"><a class="header" href="#storing-data">Storing Data</a></h1>
<p>Storing data can be done to collect the raw content for a website.</p>
<p>This allows you to upload and download the content without UTF-8 conversion. The property only appears when
setting the second param of the <code>Website</code> class constructor to true.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

class Subscription:
    def __init__(self):
        print(&quot;Subscription Created...&quot;)
    def __call__(self, page):
        print(page.url + &quot; - bytes: &quot; + str(page.raw_content))
        # do something with page.raw_content

async def main():
    website = Website(&quot;https://choosealicense.com&quot;)
    website.crawl(Subscription(), True)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="benchmarks"><a class="header" href="#benchmarks">Benchmarks</a></h1>
<p>Test url: <code>https://espn.com</code>
Mac M1 64gb 10-core CPU</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>pages</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider(rust): crawl</code></strong></td><td style="text-align: left"><code>150,387</code></td><td style="text-align: left"><code>1m</code></td></tr>
<tr><td style="text-align: left"><strong><code>spider(nodejs): crawl</code></strong></td><td style="text-align: left"><code>150,387</code></td><td style="text-align: left"><code>153s</code></td></tr>
<tr><td style="text-align: left"><strong><code>spider(python): crawl</code></strong></td><td style="text-align: left"><code>150,387</code></td><td style="text-align: left"><code>186s</code></td></tr>
<tr><td style="text-align: left"><strong><code>scrapy(python): crawl</code></strong></td><td style="text-align: left"><code>49,598</code></td><td style="text-align: left"><code>1h</code></td></tr>
<tr><td style="text-align: left"><strong><code>crawlee(nodejs): crawl</code></strong></td><td style="text-align: left"><code>18,779</code></td><td style="text-align: left"><code>30m</code></td></tr>
</tbody></table>
</div>
<p>View the latest runs on <a href="https://github.com/spider-rs/spider-py/actions/workflows/bench.yml">github</a>.</p>
<pre><code class="language-sh">-----------------------
Linux
2-core CPU
7 GB of RAM memory
-----------------------
</code></pre>
<p>Test url: <code>https://choosealicense.com</code> (small)
32 pages</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider-rs: crawl 10 samples</code></strong></td><td style="text-align: left"><code>76ms</code></td></tr>
<tr><td style="text-align: left"><strong><code>scrapy: crawl 10 samples</code></strong></td><td style="text-align: left"><code>2s</code></td></tr>
</tbody></table>
</div>
<p>Test url: <code>https://rsseau.fr</code> (medium)
211 pages</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider-rs: crawl 10 samples</code></strong></td><td style="text-align: left"><code>3s</code></td></tr>
<tr><td style="text-align: left"><strong><code>scrapy: crawl 10 samples</code></strong></td><td style="text-align: left"><code>8s</code></td></tr>
</tbody></table>
</div>
<pre><code class="language-sh">----------------------
mac Apple M1 Max
10-core CPU
64 GB of RAM memory
-----------------------
</code></pre>
<p>Test url: <code>https://choosealicense.com</code> (small)
32 pages</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider-rs: crawl 10 samples</code></strong></td><td style="text-align: left"><code>286ms</code></td></tr>
<tr><td style="text-align: left"><strong><code>scrapy: crawl 10 samples</code></strong></td><td style="text-align: left"><code>2.5s</code></td></tr>
</tbody></table>
</div>
<p>Test url: <code>https://rsseau.fr</code> (medium)
211 pages</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider-rs: crawl 10 samples</code></strong></td><td style="text-align: left"><code>2.5s</code></td></tr>
<tr><td style="text-align: left"><strong><code>scrapy: crawl 10 samples</code></strong></td><td style="text-align: left"><code>10s</code></td></tr>
</tbody></table>
</div>
<p>Test url: <code>https://a11ywatch.com</code> (medium)
648 pages</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider-rs: crawl 10 samples</code></strong></td><td style="text-align: left"><code>2s</code></td></tr>
<tr><td style="text-align: left"><strong><code>scrapy: crawl 10 samples</code></strong></td><td style="text-align: left"><code>7.7s</code></td></tr>
</tbody></table>
</div>
<p>Test url: <code>https://espn.com</code> (large)
150,387 pages</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>pages</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider-rs: crawl 10 samples</code></strong></td><td style="text-align: left"><code>150,387</code></td><td style="text-align: left"><code>186s</code></td></tr>
<tr><td style="text-align: left"><strong><code>scrapy: crawl 10 samples</code></strong></td><td style="text-align: left"><code>49,598</code></td><td style="text-align: left"><code>1h</code></td></tr>
</tbody></table>
</div>
<p>Scrapy used too much memory, crawl cancelled after an hour.</p>
<p>Note: The performance scales the larger the website and if throttling is needed. Linux benchmarks are about 10x faster than macOS for spider-rs.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
