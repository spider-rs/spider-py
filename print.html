<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>spider-py</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">spider-py</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/spider-rs/spider-py/tree/main/book" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p><code>Spider-Py</code> is the fastest web crawler and indexer written in Rust ported to Python.</p>
<ul>
<li>Concurrent</li>
<li>Streaming</li>
<li>Decentralization</li>
<li>Headless Chrome <a href="https://github.com/mattsse/chromiumoxide">Rendering</a></li>
<li>HTTP Proxies</li>
<li>Cron Jobs</li>
<li>Subscriptions</li>
<li>Blacklisting and Budgeting Depth</li>
<li>Written in <a href="https://www.rust-lang.org/">Rust</a> for speed, safety, and simplicity</li>
</ul>
<p>Spider powers some big tools and helps bring the crawling aspect to almost no downtime with the correct setup, view the <a href="https://github.com/spider-rs/spider">spider</a> project to learn more.</p>
<p>Test url: <code>https://espn.com</code></p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>pages</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider-rs(python): crawl</code></strong></td><td style="text-align: left"><code>150,387</code></td><td style="text-align: left"><code>186s</code></td></tr>
<tr><td style="text-align: left"><strong><code>scrapy(python): crawl</code></strong></td><td style="text-align: left"><code>49,598</code></td><td style="text-align: left"><code>1h</code></td></tr>
</tbody></table>
</div>
<p>The benches above were ran on a mac m1, spider on linux arm machines performs about 2-10x faster.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h1>
<p>Make sure to have python installed.</p>
<pre><code class="language-sh">pip install spider_rs
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="simple-example"><a class="header" href="#simple-example">Simple Example</a></h1>
<p>We use the <a href="https://pyo3.rs/v0.20.0/">pyo3</a> to port the Rust project to target Python.</p>
<p>There are some performance drawbacks from the addon, even still the crawls are lightning fast and efficient.</p>
<h2 id="usage"><a class="header" href="#usage">Usage</a></h2>
<p>The examples below can help get started with spider.</p>
<h3 id="basic"><a class="header" href="#basic">Basic</a></h3>
<pre><code class="language-python">import asyncio

from spider_rs import Website

async def main():
    website = Website("https://jeffmendez.com")
    website.crawl()
    print(website.get_links())

asyncio.run(main())
</code></pre>
<h3 id="events"><a class="header" href="#events">Events</a></h3>
<p>You can pass an object that could be async as param to <code>crawl</code> and <code>scrape</code>.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

class Subscription:
    def __init__(self):
        print("Subscription Created...")
    def __call__(self, page):
        print(page.url + " - status: " + str(page.status_code))

async def main():
    website = Website("https://choosealicense.com")
    website.crawl(Subscription())

asyncio.run(main())
</code></pre>
<h3 id="selector"><a class="header" href="#selector">Selector</a></h3>
<p>The <code>title</code> method allows you to extract the title of the page.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

class Subscription:
    def __init__(self):
        print("Subscription Created...")
    def __call__(self, page):
        print(page.url + " - title: " + str(page.title()))

async def main():
    website = Website("https://choosealicense.com")
    website.crawl(Subscription())
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="website"><a class="header" href="#website">Website</a></h1>
<p>The Website class is the foundations to the spider.</p>
<h2 id="builder-pattern"><a class="header" href="#builder-pattern">Builder pattern</a></h2>
<p>We use the builder pattern to configure the website for crawling.</p>
<p>*note: Replace <code>https://choosealicense.com</code> from the examples below with your website target URL.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com")
    website.crawl()
    print(website.get_links())

asyncio.run(main())
</code></pre>
<h3 id="return-page-links"><a class="header" href="#return-page-links">Return Page Links</a></h3>
<p>Return links found on the page resource.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_return_page_links(True)

asyncio.run(main())
</code></pre>
<h3 id="custom-headers"><a class="header" href="#custom-headers">Custom Headers</a></h3>
<p>Add custom HTTP headers to use when crawling/scraping.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_headers({ "authorization": "mytoken"})

asyncio.run(main())
</code></pre>
<h3 id="blacklist"><a class="header" href="#blacklist">Blacklist</a></h3>
<p>Prevent crawling a set path, url, or pattern with Regex.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_blacklist_url(["/blog", "/resume"])

asyncio.run(main())
</code></pre>
<h3 id="whitelist"><a class="header" href="#whitelist">Whitelist</a></h3>
<p>Only crawl set paths, url, or pattern with Regex.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_whitelist_url(["/licenses"])

asyncio.run(main())
</code></pre>
<h3 id="crons"><a class="header" href="#crons">Crons</a></h3>
<p>Setup a cron job that can run at any time in the background using cron-syntax.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_cron("1/5 * * * * *")

asyncio.run(main())
</code></pre>
<p>View the <a href="./cron-job.html">cron</a> section for details how to use the cron.</p>
<h3 id="budget"><a class="header" href="#budget">Budget</a></h3>
<p>Add a crawl budget that prevents crawling <code>x</code> amount of pages.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_budget({
    "*": 1,
  })

asyncio.run(main())
</code></pre>
<h3 id="subdomains"><a class="header" href="#subdomains">Subdomains</a></h3>
<p>Include subdomains in request.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_subdomains(True)

asyncio.run(main())
</code></pre>
<h3 id="tld"><a class="header" href="#tld">TLD</a></h3>
<p>Include TLDs in request.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_tld(True)

asyncio.run(main())
</code></pre>
<h3 id="chrome-remote-connection"><a class="header" href="#chrome-remote-connection">Chrome Remote Connection</a></h3>
<p>Add a chrome remote connection url. This can be a json endpoint or ws direct connection.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_chrome_connection("http://localhost:9222/json/version")

asyncio.run(main())
</code></pre>
<h3 id="external-domains"><a class="header" href="#external-domains">External Domains</a></h3>
<p>Add external domains to include with the website.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_external_domains(["https://www.myotherdomain.com"])

asyncio.run(main())
</code></pre>
<h3 id="proxy"><a class="header" href="#proxy">Proxy</a></h3>
<p>Use a proxy to crawl a website.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_proxies(["https://www.myproxy.com"])

asyncio.run(main())
</code></pre>
<h3 id="depth-limit"><a class="header" href="#depth-limit">Depth Limit</a></h3>
<p>Set the depth limit for the amount of forward pages.</p>
<pre><code class="language-ts">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_depth(3)

asyncio.run(main())
</code></pre>
<h3 id="cache"><a class="header" href="#cache">Cache</a></h3>
<p>Enable HTTP caching, this useful when using the spider on a server.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_caching(True)

asyncio.run(main())
</code></pre>
<h3 id="delays"><a class="header" href="#delays">Delays</a></h3>
<p>Add delays between pages. Defaults to none.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_delays(200)

asyncio.run(main())
</code></pre>
<h3 id="user-agent"><a class="header" href="#user-agent">User-Agent</a></h3>
<p>Use a custom User-Agent.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_user_agent("mybot/v1")

asyncio.run(main())
</code></pre>
<h3 id="request-timeout"><a class="header" href="#request-timeout">Request Timeout</a></h3>
<p>Add a request timeout per page in miliseconds. Example shows 30 seconds.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_request_timeout(30000)

asyncio.run(main())
</code></pre>
<h3 id="wait-for-idle-network"><a class="header" href="#wait-for-idle-network">Wait For Idle Network</a></h3>
<p>You can wait for the Network to become idle when using chrome. This helps load all the data from client side scripts.
The first param is whether to enable or not and the second is the duration max timeout in milliseconds.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_wait_for_idle_network(True, 12000)

asyncio.run(main())
</code></pre>
<h3 id="respect-robots"><a class="header" href="#respect-robots">Respect Robots</a></h3>
<p>Respect the robots.txt file.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_respect_robots_txt(True)

asyncio.run(main())
</code></pre>
<h3 id="collect-full-resources"><a class="header" href="#collect-full-resources">Collect Full Resources</a></h3>
<p>Collect all resources found not just valid web pages.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_full_resources(True)

asyncio.run(main())
</code></pre>
<h3 id="openai"><a class="header" href="#openai">OpenAI</a></h3>
<p>Use OpenAI to generate dynamic scripts to use with headless. Make sure to set the <code>OPENAI_API_KEY</code> env variable.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = (
        Website("https://google.com")
        .with_openai({
            "model": "gpt-3.5-turbo",
            "prompt": "Search for movies",
            "maxTokens": 300
        })
    )

asyncio.run(main())
</code></pre>
<h3 id="screenshots"><a class="header" href="#screenshots">Screenshots</a></h3>
<p>Take a screenshot of the pages on crawl when using headless chrome.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = (
        Website("https://choosealicense.com", False)
        .with_screenshot({
            "params": {
                "cdp_params": None,
                "full_page": True,
                "omit_background": False
            },
            "bytes": False,
            "save": True,
            "output_dir": None
        })
    )

asyncio.run(main())
</code></pre>
<h3 id="http2-prior-knowledge"><a class="header" href="#http2-prior-knowledge">Http2 Prior Knowledge</a></h3>
<p>Use http2 to connect if you know the website servers supports this.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_http2_prior_knowledge(True)

asyncio.run(main())
</code></pre>
<h3 id="preserve-host"><a class="header" href="#preserve-host">Preserve Host</a></h3>
<p>Preserve the HOST HTTP header.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_preserve_host_header(True)

asyncio.run(main())
</code></pre>
<h2 id="chaining"><a class="header" href="#chaining">Chaining</a></h2>
<p>You can chain all of the configs together for simple configuration.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com").with_subdomains(true).with_tlds(true).with_user_agent("mybot/v1").with_respect_robots_txt(true)

asyncio.run(main())
</code></pre>
<h2 id="raw-content"><a class="header" href="#raw-content">Raw Content</a></h2>
<p>Set the second param of the website constructor to <code>true</code> to return content without UTF-8.
This will return <code>rawContent</code> and leave <code>content</code> when using subscriptions or the Page Object.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com", True)
    website.scrape()

asyncio.run(main())
</code></pre>
<h2 id="clearing-crawl-data"><a class="header" href="#clearing-crawl-data">Clearing Crawl Data</a></h2>
<p>Use <code>website.clear</code> to remove the links visited and page data or <code>website.drain_links</code> to drain the links visited.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com")
    website.crawl()
    print(website.getLinks())
    website.clear()
    print(website.getLinks())

asyncio.run(main())
</code></pre>
<h2 id="stop-crawl"><a class="header" href="#stop-crawl">Stop crawl</a></h2>
<p>To stop a crawl you can use <code>website.stopCrawl(id)</code>, pass in the crawl id to stop a run or leave empty for all crawls to stop.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

class Subscription:
    def __init__(self):
        print("Subscription Created...")
    def __call__(self, page):
        print(page.url + " - status: " + str(page.status_code))

async def main():
    website = Website("https://choosealicense.com")
    website.crawl(Subscription())
    # sleep for 2s and stop etc
    website.stop()

asyncio.run(main())
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="page"><a class="header" href="#page">Page</a></h1>
<p>A single page on a website, useful if you need just the root url.</p>
<h2 id="new-page"><a class="header" href="#new-page">New Page</a></h2>
<p>Get a new page with content.</p>
<p>The first param is the url, followed by if subdomains should be included, and last to include TLD's in links.</p>
<p>Calling <code>page.fetch</code> is needed to get the content.</p>
<pre><code class="language-python">import asyncio
from spider_rs import Page

async def main():
    page = Page("https://choosealicense.com")
    page.fetch()

asyncio.run(main())
</code></pre>
<h2 id="page-links"><a class="header" href="#page-links">Page Links</a></h2>
<p>get all the links related to a page.</p>
<pre><code class="language-python">import asyncio
from spider_rs import Page

async def main():
    page = Page("https://choosealicense.com")
    page.fetch()
    links = page.get_links()
    print(links)
asyncio.run(main())
</code></pre>
<h2 id="page-html"><a class="header" href="#page-html">Page Html</a></h2>
<p>Get the markup for the page or HTML.</p>
<pre><code class="language-python">import asyncio
from spider_rs import Page

async def main():
    page = Page("https://choosealicense.com")
    page.fetch()
    links = page.get_html()
    print(links)

asyncio.run(main())
</code></pre>
<h2 id="page-bytes"><a class="header" href="#page-bytes">Page Bytes</a></h2>
<p>Get the raw bytes of a page to store the files in a database.</p>
<pre><code class="language-python">import asyncio
from spider_rs import Page

async def main():
    page = Page("https://choosealicense.com")
    page.fetch()
    links = page.get_bytes()
    print(links)

asyncio.run(main())
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="environment"><a class="header" href="#environment">Environment</a></h1>
<p>Env variables to adjust the project.</p>
<h2 id="chrome_url"><a class="header" href="#chrome_url">CHROME_URL</a></h2>
<p>You can set the chrome URL to connect remotely.</p>
<pre><code class="language-sh">CHROME_URL=http://localhost:9222
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="crawl"><a class="header" href="#crawl">Crawl</a></h1>
<p>Crawl a website concurrently.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://rsseau.fr")
    website.crawl()
    print(website.get_links())

asyncio.run(main())
</code></pre>
<h2 id="async-event"><a class="header" href="#async-event">Async Event</a></h2>
<p>You can pass in a async function as the first param to the crawl function for realtime updates streamed.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

class Subscription:
    def __init__(self):
        print("Subscription Created...")
    def __call__(self, page):
        print(page.url + " - status: " + str(page.status_code))

async def main():
    website = Website("https://choosealicense.com")
    website.crawl(Subscription())

asyncio.run(main())
</code></pre>
<h2 id="background"><a class="header" href="#background">Background</a></h2>
<p>You can run the request in the background and receive events with the second param set to <code>true</code>.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

class Subscription:
    def __init__(self):
        print("Subscription Created...")
    def __call__(self, page):
        print(page.url + " - status: " + str(page.status_code))

async def main():
    website = Website("https://choosealicense.com")
    website.crawl(Subscription(), True)
    # this will run instantly as the crawl is in the background

asyncio.run(main())
</code></pre>
<h2 id="subscriptions"><a class="header" href="#subscriptions">Subscriptions</a></h2>
<p>You can setup many subscriptions to run events when a crawl happens.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

class Subscription:
    def __init__(self):
        print("Subscription Created...")
    def __call__(self, page):
        print(page.url + " - status: " + str(page.status_code))

async def main():
    website = Website("https://choosealicense.com")
    website.crawl()
    subscription_id = website.subscribe(Subscription());
    website.crawl()
    website.unsubscribe(subscription_id);

asyncio.run(main())
</code></pre>
<h2 id="headless-chrome"><a class="header" href="#headless-chrome">Headless Chrome</a></h2>
<p>Headless Chrome rendering can be done by setting the third param in <code>crawl</code> or <code>scrape</code> to <code>true</code>.
It will attempt to connect to chrome running remotely if the <code>CHROME_URL</code> env variable is set with chrome launching as a fallback. Using a remote connection with <code>CHROME_URL</code> will
drastically speed up runs.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

class Subscription:
    def __init__(self):
        print("Subscription Created...")
    def __call__(self, page):
        print(page.url + " - status: " + str(page.status_code))

async def main():
    website = Website("https://choosealicense.com")
    website.crawl(Subscription(), false, True)

asyncio.run(main())
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scrape"><a class="header" href="#scrape">Scrape</a></h1>
<p>Scape a website and collect the resource data.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com")
    website.scrape()
    print(website.get_pages())
    # [ { url: "https://rsseau.fr/blog", html: "&lt;html&gt;...&lt;/html&gt;"}, ...]

asyncio.run(main())
</code></pre>
<h2 id="headless-chrome-1"><a class="header" href="#headless-chrome-1">Headless Chrome</a></h2>
<p>Headless Chrome rendering can be done by setting the third param in <code>crawl</code> or <code>scrape</code> to <code>true</code>.
It will attempt to connect to chrome running remotely if the <code>CHROME_URL</code> env variable is set with chrome launching as a fallback. Using a remote connection with <code>CHROME_URL</code> will
drastically speed up runs.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

async def main():
    website = Website("https://choosealicense.com")
    website.scrape(NULL, NULL, True)
    print(website.get_pages())
    # [ { url: "https://rsseau.fr/blog", html: "&lt;html&gt;...&lt;/html&gt;"}, ...]

asyncio.run(main())
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cron-jobs"><a class="header" href="#cron-jobs">Cron Jobs</a></h1>
<p>Use a cron job that can run any time of day to gather website data.</p>
<pre><code class="language-python">import asyncio
from spider_rs import Website

class Subscription:
    def __init__(self):
        print("Cron Created...")
    def __call__(self, page):
        print(page.url + " - status: " + str(page.status_code))

async def main():
    website = Website("https://choosealicense.com").with_cron("1/5 * * * * *").build()
    handle = await website.run_cron(Subscription());

asyncio.run(main())
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="storing-data"><a class="header" href="#storing-data">Storing Data</a></h1>
<p>Storing data can be done to collect the raw content for a website.</p>
<p>This allows you to upload and download the content without UTF-8 conversion. The property only appears when
setting the second param of the <code>Website</code> class constructor to true.</p>
<pre><code class="language-py">import asyncio
from spider_rs import Website

class Subscription:
    def __init__(self):
        print("Subscription Created...")
    def __call__(self, page):
        print(page.url + " - bytes: " + str(page.raw_content))
        # do something with page.raw_content

async def main():
    website = Website("https://choosealicense.com")
    website.crawl(Subscription(), True)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="benchmarks"><a class="header" href="#benchmarks">Benchmarks</a></h1>
<p>Test url: <code>https://espn.com</code>
Mac M1 64gb 10-core CPU</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>pages</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider(rust): crawl</code></strong></td><td style="text-align: left"><code>150,387</code></td><td style="text-align: left"><code>1m</code></td></tr>
<tr><td style="text-align: left"><strong><code>spider(nodejs): crawl</code></strong></td><td style="text-align: left"><code>150,387</code></td><td style="text-align: left"><code>153s</code></td></tr>
<tr><td style="text-align: left"><strong><code>spider(python): crawl</code></strong></td><td style="text-align: left"><code>150,387</code></td><td style="text-align: left"><code>186s</code></td></tr>
<tr><td style="text-align: left"><strong><code>scrapy(python): crawl</code></strong></td><td style="text-align: left"><code>49,598</code></td><td style="text-align: left"><code>1h</code></td></tr>
<tr><td style="text-align: left"><strong><code>crawlee(nodejs): crawl</code></strong></td><td style="text-align: left"><code>18,779</code></td><td style="text-align: left"><code>30m</code></td></tr>
</tbody></table>
</div>
<p>View the latest runs on <a href="https://github.com/spider-rs/spider-py/actions/workflows/bench.yml">github</a>.</p>
<pre><code class="language-sh">-----------------------
Linux
2-core CPU
7 GB of RAM memory
-----------------------
</code></pre>
<p>Test url: <code>https://choosealicense.com</code> (small)
32 pages</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider-rs: crawl 10 samples</code></strong></td><td style="text-align: left"><code>76ms</code></td></tr>
<tr><td style="text-align: left"><strong><code>scrapy: crawl 10 samples</code></strong></td><td style="text-align: left"><code>2s</code></td></tr>
</tbody></table>
</div>
<p>Test url: <code>https://rsseau.fr</code> (medium)
211 pages</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider-rs: crawl 10 samples</code></strong></td><td style="text-align: left"><code>3s</code></td></tr>
<tr><td style="text-align: left"><strong><code>scrapy: crawl 10 samples</code></strong></td><td style="text-align: left"><code>8s</code></td></tr>
</tbody></table>
</div>
<pre><code class="language-sh">----------------------
mac Apple M1 Max
10-core CPU
64 GB of RAM memory
-----------------------
</code></pre>
<p>Test url: <code>https://choosealicense.com</code> (small)
32 pages</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider-rs: crawl 10 samples</code></strong></td><td style="text-align: left"><code>286ms</code></td></tr>
<tr><td style="text-align: left"><strong><code>scrapy: crawl 10 samples</code></strong></td><td style="text-align: left"><code>2.5s</code></td></tr>
</tbody></table>
</div>
<p>Test url: <code>https://rsseau.fr</code> (medium)
211 pages</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider-rs: crawl 10 samples</code></strong></td><td style="text-align: left"><code>2.5s</code></td></tr>
<tr><td style="text-align: left"><strong><code>scrapy: crawl 10 samples</code></strong></td><td style="text-align: left"><code>10s</code></td></tr>
</tbody></table>
</div>
<p>Test url: <code>https://a11ywatch.com</code> (medium)
648 pages</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider-rs: crawl 10 samples</code></strong></td><td style="text-align: left"><code>2s</code></td></tr>
<tr><td style="text-align: left"><strong><code>scrapy: crawl 10 samples</code></strong></td><td style="text-align: left"><code>7.7s</code></td></tr>
</tbody></table>
</div>
<p>Test url: <code>https://espn.com</code> (large)
150,387 pages</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left"><code>libraries</code></th><th style="text-align: left"><code>pages</code></th><th style="text-align: left"><code>speed</code></th></tr></thead><tbody>
<tr><td style="text-align: left"><strong><code>spider-rs: crawl 10 samples</code></strong></td><td style="text-align: left"><code>150,387</code></td><td style="text-align: left"><code>186s</code></td></tr>
<tr><td style="text-align: left"><strong><code>scrapy: crawl 10 samples</code></strong></td><td style="text-align: left"><code>49,598</code></td><td style="text-align: left"><code>1h</code></td></tr>
</tbody></table>
</div>
<p>Scrapy used too much memory, crawl cancelled after an hour.</p>
<p>Note: The performance scales the larger the website and if throttling is needed. Linux benchmarks are about 10x faster than macOS for spider-rs.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
